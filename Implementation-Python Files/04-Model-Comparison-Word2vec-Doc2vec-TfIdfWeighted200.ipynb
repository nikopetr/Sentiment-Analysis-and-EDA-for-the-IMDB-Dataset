{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of Different Word Embeddings on Text Classification\n",
    "\n",
    "## Compared among word2vec, TF-IDF weighted, GloVe and doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-31 11:36:55.321478: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-10-31 11:36:55.321497: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Data libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "\n",
    "# Word2Vec libraries\n",
    "import gensim\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "# Gensim utils for glove2word2vec\n",
    "from gensim.test.utils import get_tmpfile, datapath\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "from collections import namedtuple, defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Classifiers that are going to be used\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Evaluation metrics and reports\n",
    "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix\n",
    "# Grid-search and crossvalidation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import timeit\n",
    "\n",
    "# NLP/language/string Imports\n",
    "import spacy\n",
    "import en_core_web_lg\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import contractions\n",
    "\n",
    "# Imports for files operations\n",
    "import os\n",
    "import pickle\n",
    "from numpy import genfromtxt # In order to load a .csv file to a numpy array object\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "pd.set_option('max_colwidth',1000)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "VECTOR_SIZE = 200\n",
    "\n",
    "PICKLE_FILE = 'all_doc_news.pickle'\n",
    "WORD2VEC_FILE = \"word2vec_reviews_new.model\"\n",
    "MEAN_EMBEDDING_FILE = 'doc_vec_new.csv'\n",
    "TFIDF_EMBEDDING_FILE = 'tfidf_doc_vec_new.csv'\n",
    "GLOVE_EMBEDDING_FILE = 'glove_doc_vec_new.csv'\n",
    "DOC2VEC_EMBEDDING_FILE = 'dm_doc_vec_new.csv'\n",
    "\n",
    "random_state = 420\n",
    "\n",
    "# Specify train/valid/test sizes\n",
    "TEST_SIZE = 0.2\n",
    "VALID_SIZE = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Incentive\n",
    "\n",
    "It's been a while not able to write new posts, so sad, but now finally I am back again to share some of the knowledge I've just acquired. This time is about NLP. \n",
    "\n",
    "As a fresh rookie in NLP, I'd like to play around and test out how different methods of creating doc vector perform on text classification. This post will be highly focused on feature engineering side, that is word vectorization, and less on modeling. Thus, without further due, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brief Introduction\n",
    "\n",
    "The word embeddings being investigated here are word2vec, TF-IDF weighted word2vec, pre-train GloVe word2vec and doc2vec. The packages needed are Gensim, Spacy and Scikit-Learn. Spacy is used in doc preprocessing, including stop word removal and custom token selection based on its part of speech. Gensim is heavily applied for training word2vec and doc2vec, and lastly, Scikit-Learn is for classifier building and training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Summary\n",
    "\n",
    "After a series of comparison on different word embedding/averaging methods, it turns out that custom-trained word embedding and its averaging method, either simple mean or TF-IDF weighted has the best performance, while on the contrary, GloVe word embedding or custom-trained Doc2vec perform slightly worse than the former word embedding.\n",
    "\n",
    "Besides, even if we try to concatenate both word2vec and doc2vec as a whole feature set, it performs equally the same to just using averaging word embedding alone. In other words, no need to use both word2vec and doc2vec at the same time.\n",
    "\n",
    "\n",
    "| WordEmbedding Method        | F1 Score - Training | F1 Score - Testing | Accuracy - Training | Accuracy - Testing |\n",
    "| :---:                       | :---:               | :---:              | :---:               | :---:              |\n",
    "| Mean Word2vec               | 0.82                | 0.81               | 0.82                | 0.81               |\n",
    "| Tf-Idf Mean Word2vec        | 0.82                | 0.81               | 0.82                | 0.81               |\n",
    "| GloVe Mean Word2vec         | 0.72                | 0.71               | 0.73                | 0.72               |\n",
    "| PV-DM Doc2vec               | 0.79                | 0.78               | 0.79                | 0.78               |\n",
    "| Tf-Idf Word2vec + Doc2vec   | 0.84                | 0.81               | 0.85                | 0.82               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special Credits to the Following Posts and Authors\n",
    "\n",
    "In creating my **python class object** used for text preprocessing, I referred from these well-written posts.\n",
    "\n",
    "* The post [\"Text Classification with Word2vec\"](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/) by nadbor demos how to write your own class to compute average word embedding for doc, either simple averaging or TF-IDF weighted one.\n",
    "\n",
    "* [\"Multi-Class Text Classification Model Comparison and Selection\"](https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568) by Susan Li teaches me how to write beautiful averaging function for word embedding.\n",
    "\n",
    "* This tutorial [\"Gensim Doc2vec Tutorial on the IMDB Sentiment Dataset\"](https://github.com/RaRe-Technologies/gensim/blob/3c3506d51a2caf6b890de3b1b32a8b85f7566ca5/docs/notebooks/doc2vec-IMDB.ipynb) has step by step guidance on how to create doc2vec via Gensim.\n",
    "\n",
    "* [\"Distributed representations of sentences and documents\"](https://blog.acolyer.org/2016/06/01/distributed-representations-of-sentences-and-documents/) by Le & Mikolov presents a clear and easy-to-understand explanation on what's going under doc2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset I am gonna use here is consumer complaints dataset on financial product/service as referred from [the post](https://towardsdatascience.com/multi-class-text-classification-with-doc2vec-logistic-regression-9da9947b43f4). The dataset is collected and published by [US GOV CFPB](https://catalog.data.gov/dataset/consumer-complaint-database), while we can also download the dataset from [Kaggle](https://www.kaggle.com/cfpb/us-consumer-finance-complaints).\n",
    "\n",
    "The original dataset contains more than 500 thousands records, and columns include product, sub_product, issue, consumer_complaint_narrative, and company_response_to_consumer etc.. We will just use **product** as text label and **consumer_complaint_narrative** as text itself. After dropping rows of missing values on consumer complaint we are left with around 60 thousands records. In order to lessen the computing pressure, I will just experiment on the first 25 thousands records only."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in file.\n",
    "df = pd.read_csv('IMDB Dataset.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Familiar with Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get familiar with dataset.\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>50000</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stimulating and captivating, always keeping the viewer peeking around the corner to see what was coming up next. She is as down to earth and as personable as you get, like one of us which made the show all the more enjoyable. Special guests, who are friends as well made for a nice surprise too. Loved the 'first' theme and that the audience was invited to play along too. I must admit I was shocked to see her come in under her time limits on a few things, but she did it and by golly I'll be writing those recipes down. Saving time in the kitchen means more time with family. Those who haven't tuned in yet, find out what channel and the time, I assure you that you won't be disappointed.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          review  \\\n",
       "count                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      50000   \n",
       "unique                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     49582   \n",
       "top     Loved today's show!!! It was a variety and not solely cooking (which would have been great too). Very stimulating and captivating, always keeping the viewer peeking around the corner to see what was coming up next. She is as down to earth and as personable as you get, like one of us which made the show all the more enjoyable. Special guests, who are friends as well made for a nice surprise too. Loved the 'first' theme and that the audience was invited to play along too. I must admit I was shocked to see her come in under her time limits on a few things, but she did it and by golly I'll be writing those recipes down. Saving time in the kitchen means more time with family. Those who haven't tuned in yet, find out what channel and the time, I assure you that you won't be disappointed.   \n",
       "freq                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           5   \n",
       "\n",
       "       sentiment  \n",
       "count      50000  \n",
       "unique         2  \n",
       "top     positive  \n",
       "freq       25000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>49582</td>\n",
       "      <td>49582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>49582</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.&lt;br /&gt;&lt;br /&gt;It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.&lt;br /&gt;&lt;br /&gt;I would say the main appeal of the show is due to the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>24884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         review  \\\n",
       "count                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     49582   \n",
       "unique                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    49582   \n",
       "top     One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the...   \n",
       "freq                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1   \n",
       "\n",
       "       sentiment  \n",
       "count      49582  \n",
       "unique         2  \n",
       "top     positive  \n",
       "freq       24884  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select duplicate rows except first occurrence based on all columns\n",
    "df = df.drop_duplicates()\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter on Text and Label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**Chart of Label Frequency**</mark>\n",
    "\n",
    "Now, let's see how frequency distributed among each label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    24884\n",
       "negative    24698\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Distribution of target variable.\n",
    "display(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**Sample of Dataset**</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo of review and its sentiment example...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.&lt;br /&gt;&lt;br /&gt;The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.&lt;br /&gt;&lt;br /&gt;It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.&lt;br /&gt;&lt;br /&gt;I would say the main appeal of the show is due to the...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. &lt;br /&gt;&lt;br /&gt;The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. &lt;br /&gt;&lt;br /&gt;The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.&lt;br /&gt;&lt;br /&gt;This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.&lt;br /&gt;&lt;br /&gt;This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet &amp; his parents are fighting all the time.&lt;br /&gt;&lt;br /&gt;This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.&lt;br /&gt;&lt;br /&gt;OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing &amp; arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.&lt;br /&gt;&lt;br /&gt;3 out of 10 just for the well playing parents &amp; descent dialogs. As for the shots with Jake: just ignore them.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. &lt;br /&gt;&lt;br /&gt;This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.&lt;br /&gt;&lt;br /&gt;The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case wi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    review  \\\n",
       "0  One of the other reviewers has mentioned that after watching just 1 Oz episode you'll be hooked. They are right, as this is exactly what happened with me.<br /><br />The first thing that struck me about Oz was its brutality and unflinching scenes of violence, which set in right from the word GO. Trust me, this is not a show for the faint hearted or timid. This show pulls no punches with regards to drugs, sex or violence. Its is hardcore, in the classic use of the word.<br /><br />It is called OZ as that is the nickname given to the Oswald Maximum Security State Penitentary. It focuses mainly on Emerald City, an experimental section of the prison where all the cells have glass fronts and face inwards, so privacy is not high on the agenda. Em City is home to many..Aryans, Muslims, gangstas, Latinos, Christians, Italians, Irish and more....so scuffles, death stares, dodgy dealings and shady agreements are never far away.<br /><br />I would say the main appeal of the show is due to the...   \n",
       "1   A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master's of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional 'dream' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell's murals decorating every surface) are terribly well done.   \n",
       "2                                                                           I thought this was a wonderful way to spend time on a too hot summer weekend, sitting in the air conditioned theater and watching a light-hearted comedy. The plot is simplistic, but the dialogue is witty and the characters are likable (even the well bread suspected serial killer). While some may be disappointed when they realize this is not Match Point 2: Risk Addiction, I thought it was proof that Woody Allen is still fully in control of the style many of us have grown to love.<br /><br />This was the most I'd laughed at one of Woody's comedies in years (dare I say a decade?). While I've never been impressed with Scarlet Johanson, in this she managed to tone down her \"sexy\" image and jumped right into a average, but spirited young woman.<br /><br />This may not be the crown jewel of his career, but it was wittier than \"Devil Wears Prada\" and more interesting than \"Superman\" a great comedy to go see with friends.   \n",
       "3                                                                                                                                                                                                                                                             Basically there's a family where a little boy (Jake) thinks there's a zombie in his closet & his parents are fighting all the time.<br /><br />This movie is slower than a soap opera... and suddenly, Jake decides to become Rambo and kill the zombie.<br /><br />OK, first of all when you're going to make a film you must Decide if its a thriller or a drama! As a drama the movie is watchable. Parents are divorcing & arguing like in real life. And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots.<br /><br />3 out of 10 just for the well playing parents & descent dialogs. As for the shots with Jake: just ignore them.   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is a visually stunning film to watch. Mr. Mattei offers us a vivid portrait about human relations. This is a movie that seems to be telling us what money, power and success do to people in the different situations we encounter. <br /><br />This being a variation on the Arthur Schnitzler's play about the same theme, the director transfers the action to the present time New York where all these different characters meet and connect. Each one is connected in one way, or another to the next person, but no one seems to know the previous point of contact. Stylishly, the film has a sophisticated luxurious look. We are taken to see how these people live and the world they live in their own habitat.<br /><br />The only thing one gets out of all these souls in the picture is the different stages of loneliness each one inhabits. A big city is not exactly the best place in which human relations find sincere fulfillment, as one discerns is the case wi...   \n",
       "\n",
       "  sentiment  \n",
       "0  positive  \n",
       "1  positive  \n",
       "2  positive  \n",
       "3  negative  \n",
       "4  positive  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Demo of review and its sentiment example...')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the first step -Doc Preprocessing. Before we create our own word embedding based on the input texts, we need to preprocess the text so that it complies with the input format as Gensim requires. It involves multiple steps starting from word tokenization, bi-gram detection, lemmatization etc..\n",
    "\n",
    "Here, I wrote a python class called **DocProcess**. This class implements all the nitty-gritty jobs mentioned above for us under the hood, such as:\n",
    "\n",
    "1. First, the class takes in a series of texts, then tokenizes the text and removes all punctuations.\n",
    "\n",
    "2. It has the option build_bi, meaning whether to build up bi-gram, function adopted from Gensim. The default is False, if option build_bi is set to True, then the class will train a bi-gram detector and create bi-gram words for the text.\n",
    "\n",
    "3. Now, all the processed tokens are concatenated back to form a sentence again.\n",
    "\n",
    "4. The texts are tokenized once again, but this time, both **stop words** and **parts of speech** that are not allowed in the text will be removed and all tokens are **lemmatized**. These tokens are stored as `self.doc_words` - list of the tokens for each text(doc).\n",
    "\n",
    "5. Finally, these `self.doc_words` are wrapped up into **TaggedDocument**, a object type in Gensim for later use in doc2vec training. It's stored in `self.tagdocs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/nick/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stop_words.remove('not')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wasn', 'on', 'am', 'that', \"you'll\", 'does', 'do', 'has', 'nor', 'he', 'through', 'then', 'theirs', \"you're\", 'for', \"wouldn't\", 'further', 'before', \"doesn't\", 'ours', 'from', 'them', 'him', 'there', 'haven', 'needn', 'doesn', 'just', \"don't\", 'against', 's', 'at', 'the', 'did', 'why', 'not', 'herself', 'who', 'once', 'ma', 'a', 'because', 'no', 'so', 'what', \"mightn't\", 'm', 'having', 'didn', 'is', 'can', 'hadn', 'mightn', 'which', 'won', 'hasn', 'ourselves', 'my', \"isn't\", 'it', \"shan't\", \"she's\", 'now', 'about', 'i', 'shouldn', 'few', 'with', 'after', 'her', \"wasn't\", 'off', 'doing', \"won't\", \"couldn't\", 'to', 'some', 'under', \"that'll\", 'when', 'between', 'yourselves', 're', \"aren't\", \"it's\", 'than', 'but', 'how', 'down', 'own', 'she', 'ain', 'been', 'in', 'me', 'these', 'those', 'until', 'here', 'if', 'very', \"you've\", 'were', 'couldn', 'only', 'mustn', 'whom', 'this', 'are', 'll', 'y', \"haven't\", \"you'd\", 'his', 'themselves', 'you', 'too', 'wouldn', 'same', 'aren', 'have', 'had', 'an', 'himself', 'our', 'out', 'will', 'of', 'over', 'yours', 'most', 'myself', 'don', 'while', 'both', 'yourself', 'all', 'we', 'itself', 'each', 'be', \"should've\", 'they', \"hasn't\", 'where', 'above', 'during', 'isn', 'by', 'weren', \"didn't\", \"hadn't\", 'its', 'and', 've', 'your', 'below', 'any', \"mustn't\", 'again', \"shouldn't\", 'was', 'being', 'd', \"needn't\", 'shan', \"weren't\", 'such', 'more', 't', 'o', 'into', 'or', 'should', 'their', 'hers', 'other', 'as', 'up'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 190 reviews which include atleast one url\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.4024737926809745, pvalue=0.1607801273205851)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def check_url(text):\n",
    "\n",
    "# Regular expression for url\n",
    "\n",
    "    re_equ = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?]))\"\n",
    "\n",
    "    ck_url = re.findall(re_equ, text)\n",
    "\n",
    "    if ck_url:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "       \n",
    "\n",
    "df_temp = df.copy()\n",
    "df_temp['contains_url'] = df_temp['review'].apply(check_url)\n",
    "\n",
    "print(\"There are\", sum(df_temp['contains_url']), \"reviews which include atleast one url\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "tips = sns.load_dataset(\"tips\")\n",
    "ax = sns.barplot(x=\"sentiment\", y=\"contains_url\", data=df_temp)\n",
    "\n",
    "x_pos = df_temp[df_temp['sentiment'] == 'positive']['contains_url']\n",
    "x_neg = df_temp[df_temp['sentiment'] == 'negative']['contains_url']\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# two-side t-test with sample with identical means\n",
    "stats.ttest_ind(x_pos, x_neg,equal_var=False)\n",
    "# The t-test shows that a p-value larger than a chosen threshold (e.g. 5% or 1%) indicates that our observation is not so unlikely to have occurred by chance. Therefore, we do not reject the null hypothesis of equal population means, and therefore since only a few reviews include links-urls it is not beneficial to keep a token for the urls in the tokenization process. \n",
    "# Thus, all urls will be thrown away during the pre-processing phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The t-test shows that a p-value larger than a chosen threshold (e.g. 5% or 1%) indicates that our observation is not so unlikely to have occurred by chance. Therefore, we do not reject the null hypothesis of equal population means, and therefore since only a few reviews include links-urls it is not beneficial to keep a token for the urls in the tokenization process. Thus, all urls will be thrown away during the pre-processing phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle with the documents-reviews already exists\n"
     ]
    }
   ],
   "source": [
    "def expand_contractions(text):\n",
    "\treturn contractions.fix(text)\n",
    "\n",
    "class DocPreprocess(object):\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\t\t\t nlp,\n",
    "\t\t\t\t stop_words,\n",
    "\t\t\t\t docs,\n",
    "\t\t\t\t labels,\n",
    "\t\t\t\t build_bi=False,\n",
    "\t\t\t\t min_count=5,\n",
    "\t\t\t\t threshold=10,\n",
    "\t\t\t\t allowed_postags=['ADV', 'VERB', 'ADJ', 'NOUN', 'PROPN', 'NUM']):\n",
    "# \t\t\t\t not_allowed_postags = ['AUX', 'SCONJ', 'PUNCT', 'X', 'SYM'] # There are more\n",
    "\n",
    "\t\tself.nlp = nlp  # spacy nlp object\n",
    "\t\tself.stop_words = stop_words  # spacy.lang.en.stop_words.STOP_WORDS\n",
    "\t\tself.docs = docs  # docs must be list or series of documents\n",
    "\t\tself.labels = labels # labels must be list or or numpy array or series of labels\n",
    "\t\tself.doc_ids = np.arange(len(docs))\n",
    "\t\tself.parsed_docs = [BeautifulSoup(doc, \"html.parser\").get_text() for doc in self.docs] # Removes html tags\n",
    "\t\tself.parsed_docs = [re.sub(r'http\\S+', '', doc) for doc in self.parsed_docs] #remove urls\n",
    "\t\tself.parsed_docs = [re.sub(r'www.\\S+', '', doc) for doc in self.parsed_docs] #remove urls starting with 'www.'\n",
    "\t\tself.parsed_docs = [expand_contractions(doc) for doc in self.parsed_docs] # Expanding chatwords and contracts clearing contractions\n",
    "\t\t# This lowercases, tokenizes and remove words that their len is greater than 15 or lower than 2 (e.g. 'I')\n",
    "\t\tself.simple_doc_tokens = [gensim.utils.simple_preprocess(doc, deacc=True) for doc in self.parsed_docs]\n",
    "\n",
    "\t\tif build_bi:\n",
    "\t\t\tself.bi_detector = self.build_bi_detect(self.simple_doc_tokens, min_count=min_count, threshold=threshold)\n",
    "\t\t\tself.new_docs = self.make_bigram_doc(self.bi_detector, self.simple_doc_tokens)\n",
    "\t\telse:\n",
    "\t\t\tself.new_docs = self.make_simple_doc(self.simple_doc_tokens)\n",
    "\t\tself.doc_words = [self.lemmatize(doc, allowed_postags=allowed_postags) for doc in self.new_docs]\n",
    "\t\tself.tagdocs = [TaggedDocument(words=words, tags=[tag]) for words, tag in zip(self.doc_words, self.doc_ids)]\n",
    "\n",
    "\tdef build_bi_detect(self, simple_doc_tokens, min_count, threshold):\n",
    "\t\tbi_ = gensim.models.phrases.Phrases(simple_doc_tokens, min_count=min_count, threshold=threshold)\n",
    "\t\tbi_detector = gensim.models.phrases.Phraser(bi_)  # wrapper enhance efficiency\n",
    "\t\treturn bi_detector\n",
    "\n",
    "    \n",
    "\tdef make_simple_doc(self, simple_doc_tokens):\n",
    "\t\tsimple_docs = []\n",
    "\t\tfor doc_tokens in simple_doc_tokens:\n",
    "\t\t\tsimple = \" \".join(doc_tokens)  # concatenate back to a sentence\n",
    "\t\t\tsimple_docs.append(simple)\n",
    "\t\treturn simple_docs\n",
    "    \n",
    "\n",
    "\tdef make_bigram_doc(self, bi_detector, simple_doc_tokens):\n",
    "\t\tbi_doc_tokens = [bi_detector[doc_tokens] for doc_tokens in simple_doc_tokens]\n",
    "\t\tbi_docs = []\n",
    "\t\tfor bi_tokens in bi_doc_tokens:\n",
    "\t\t\tbi_doc = \" \".join(bi_tokens)  # concatenate back to a sentence\n",
    "\t\t\tbi_docs.append(bi_doc)\n",
    "\t\treturn bi_docs\n",
    "\n",
    "\n",
    "\tdef lemmatize(self, doc, allowed_postags):\n",
    "\t\t\"\"\"\n",
    "\t\tLemmatize words and remove stop_words.\n",
    "\t\t:param doc: text\n",
    "\t\t:param allowed_postags: list of pos tags\n",
    "\t\t:return:\n",
    "\t\t\tlist of tokens\n",
    "\t\t\"\"\"\n",
    "\t\tdoc = self.nlp(doc)\n",
    "\t\ttokens = [token.lemma_ for token in doc if (token.pos_ in allowed_postags) and (token.text not in self.stop_words)]\n",
    "\t\treturn tokens\n",
    "\n",
    "# DELETE THIS LINE \n",
    "# preprocessed_docs = DocPreprocess(nlp, stop_words, df['review'], df['sentiment'], build_bi=True)\n",
    "if not os.path.exists('./' + PICKLE_FILE):\n",
    "    preprocessed_docs = DocPreprocess(nlp, stop_words, df['review'], df['sentiment'], build_bi=True)\n",
    "else:\n",
    "    print(\"Pickle with the documents-reviews already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading documents from pickle\n"
     ]
    }
   ],
   "source": [
    "# Saving to pickle\n",
    "\n",
    "if os.path.exists('./' + PICKLE_FILE):\n",
    "    # Read the saved docs as a pickle.\n",
    "    print('Reading documents from pickle')\n",
    "    with open(os.path.join(PICKLE_FILE), 'rb') as f:\n",
    "        preprocessed_docs = pickle.load(f)\n",
    "else:  \n",
    "    # Save preprocessed_docs as a pickle.\n",
    "    print('Saving documents as a pickle')\n",
    "    with open(os.path.join(PICKLE_FILE), 'wb') as f:\n",
    "        pickle.dump(preprocessed_docs, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Struacture of Preprocessed Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "__main__.DocPreprocess"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Type of object preprocessed documents\n",
    "type(preprocessed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49582"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(preprocessed_docs.tagdocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tagged document object: \n",
      "TaggedDocument(['one', 'other_reviewer', 'mention', 'after_watche', 'episode', 'hook', 'right', 'happen', 'first', 'thing', 'struck_me', 'oz', 'brutality', 'unflinching', 'scene', 'violence', 'set', 'right', 'word', 'go', 'trust_me', 'show', 'faint_hearte', 'timid', 'show', 'pull', 'no_punche', 'regard', 'drug', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word', 'call', 'oz', 'nickname', 'give', 'oswald', 'maximum_security', 'state', 'penitentary', 'focuses_mainly', 'experimental', 'section', 'prison', 'cell', 'glass', 'front', 'face', 'inwards', 'privacy', 'high', 'agenda', 'city', 'home', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'scuffle', 'death', 'stare', 'dodgy', 'dealing', 'shady', 'agreement', 'never', 'far_away', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'go', 'show', 'dare', 'forget', 'pretty', 'picture', 'paint', 'mainstream_audience', 'forget', 'charm', 'forget', 'romance', 'mess', 'first', 'episode', 'ever', 'see', 'struck_me', 'nasty', 'surreal', 'say', 'ready', 'watch', 'developed', 'taste', 'oz', 'get', 'accustomed', 'high', 'level', 'graphic_violence', 'violence', 'injustice', 'crooked', 'guard', 'sell', 'nickel', 'inmate', 'kill', 'order', 'get', 'away', 'well_mannere', 'middle_class', 'inmate', 'prison', 'bitch', 'due', 'lack', 'street', 'skill', 'prison', 'experience', 'watch', 'oz', 'become', 'comfortable', 'uncomfortable', 'view', 'get', 'touch', 'darker_side'], [0]) \n",
      "\n",
      "Words of the document:\n",
      "['one', 'other_reviewer', 'mention', 'after_watche', 'episode', 'hook', 'right', 'happen', 'first', 'thing', 'struck_me', 'oz', 'brutality', 'unflinching', 'scene', 'violence', 'set', 'right', 'word', 'go', 'trust_me', 'show', 'faint_hearte', 'timid', 'show', 'pull', 'no_punche', 'regard', 'drug', 'sex', 'violence', 'hardcore', 'classic', 'use', 'word', 'call', 'oz', 'nickname', 'give', 'oswald', 'maximum_security', 'state', 'penitentary', 'focuses_mainly', 'experimental', 'section', 'prison', 'cell', 'glass', 'front', 'face', 'inwards', 'privacy', 'high', 'agenda', 'city', 'home', 'many', 'aryans', 'muslims', 'gangstas', 'latinos', 'christians', 'italians', 'irish', 'scuffle', 'death', 'stare', 'dodgy', 'dealing', 'shady', 'agreement', 'never', 'far_away', 'say', 'main', 'appeal', 'show', 'due', 'fact', 'go', 'show', 'dare', 'forget', 'pretty', 'picture', 'paint', 'mainstream_audience', 'forget', 'charm', 'forget', 'romance', 'mess', 'first', 'episode', 'ever', 'see', 'struck_me', 'nasty', 'surreal', 'say', 'ready', 'watch', 'developed', 'taste', 'oz', 'get', 'accustomed', 'high', 'level', 'graphic_violence', 'violence', 'injustice', 'crooked', 'guard', 'sell', 'nickel', 'inmate', 'kill', 'order', 'get', 'away', 'well_mannere', 'middle_class', 'inmate', 'prison', 'bitch', 'due', 'lack', 'street', 'skill', 'prison', 'experience', 'watch', 'oz', 'become', 'comfortable', 'uncomfortable', 'view', 'get', 'touch', 'darker_side'] \n",
      "\n",
      "Sentiment-label of tagged document: \n",
      "positive \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Example of a document\n",
    "doc_id = 0 # e.g. id = 3 \n",
    "\n",
    "print('A tagged document object: ')\n",
    "print(preprocessed_docs.tagdocs[doc_id], '\\n')\n",
    "\n",
    "print('Words of the document:')\n",
    "print(preprocessed_docs.doc_words[doc_id], '\\n')\n",
    "\n",
    "print('Sentiment-label of tagged document: ')\n",
    "print(preprocessed_docs.labels.iloc[doc_id], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the basic bow and TFidf models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "# # Getting the features (tokens-words/ngrams)\n",
    "# X = preprocessed_docs.doc_words\n",
    "\n",
    "# # Labeling the sentient data\n",
    "# y = LabelBinarizer().fit_transform(df['sentiment']).ravel()\n",
    "\n",
    "# # Notice how we used stratification over the train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=random_state, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Disabling lowercase since the data was already lowercased\n",
    "# # Using deaful utf-8 encoding as it was mentioned in the lectures\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# vectorizer = CountVectorizer(analyzer=lambda x: x, lowercase=False, encoding='utf-8')\n",
    "\n",
    "# X_train_bow = vectorizer.fit_transform(X_train)\n",
    "# print('X_train_bow:', X_train_bow.shape)\n",
    "# X_test_bow = vectorizer.transform(X_test)\n",
    "# print('X_test_bow:', X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Training the model\n",
    "# lr_2 = LogisticRegression(penalty='l2', max_iter=500, random_state=random_state,verbose=10)\n",
    "\n",
    "# # Fitting the model for TF-IDFfrom sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Training the model\n",
    "# lr_1 = LogisticRegression(penalty='l2', max_iter=500, random_state=random_state)\n",
    "\n",
    "# # Fitting the model for Bag of words\n",
    "# lr_bow = lr_1.fit(X_train_bow, y_train)\n",
    "# # Predicting the test data for Bag of words\n",
    "# lr_bow_predict = lr_1.predict(X_test_bow)\n",
    "# # Accuracy score for bag of word\n",
    "# lr_bow_score = accuracy_score(y_test,lr_bow_predict)\n",
    "# print(\"lr_bow_score :\", lr_bow_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Disabling lowercase since the data was already lowercased\n",
    "# # Using deaful utf-8 encoding as it was mentioned in the lectures\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(analyzer=lambda x: x, lowercase=False, encoding='utf-8')\n",
    "\n",
    "# X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "# print('X_train_tfidf:', X_train_tfidf.shape)\n",
    "# X_test_tfidf= vectorizer.transform(X_test)\n",
    "# print('X_test_tfidf:', X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# # Training the model\n",
    "# lr_2 = LogisticRegression(penalty='l2', max_iter=500, random_state=random_state,verbose=10)\n",
    "\n",
    "# # Fitting the model for TF-IDFfrom sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Training the model\n",
    "# lr_1 = LogisticRegression(penalty='l2', max_iter=500, random_state=random_state)\n",
    "\n",
    "# # Fitting the model for tfidf\n",
    "# lr_tfidf = lr_1.fit(X_train_tfidf, y_train)\n",
    "# # Predicting the test data for tf-idf\n",
    "# lr_tfidf_predict = lr_1.predict(X_test_tfidf)\n",
    "# # Accuracy score for tfidf\n",
    "# lr_tfidf_score = accuracy_score(y_test,lr_tfidf_predict)\n",
    "# print(\"lr_tfidf_score :\", lr_tfidf_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Returns the k most frequent n-grams from the given corpus\n",
    "# def get_top_k_ngrams(corpus, k, bi_only=False):\n",
    "#     vec = CountVectorizer(analyzer=lambda x: x, lowercase=False, encoding='utf-8').fit(corpus)\n",
    "\n",
    "#     bag_of_words = vec.transform(corpus)\n",
    "#     sum_words = bag_of_words.sum(axis=0) \n",
    "    \n",
    "#     if bi_only:\n",
    "#         words_freq = sorted([(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items() if \"_\" in word],\n",
    "#                             key = lambda x: x[1], reverse=True)\n",
    " \n",
    "#     else:\n",
    "#         words_freq = sorted([(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()], key = lambda x: x[1], reverse=True)\n",
    "\n",
    "#     return words_freq[:k]\n",
    "\n",
    "\n",
    "# import plotly.express as px\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# # Returns a barchart figure with the representation of the k most frequent n-grams\n",
    "# def get_barchart_for_top_k_ngrams(df, k=10, sentiment=None, x_axis_text=\"ngram\", bi_only=False):\n",
    "\n",
    "#   if sentiment == \"\":\n",
    "#     # Selecting all sentiments\n",
    "#     most_common_grams = get_top_k_ngrams(corpus=df.review,k=k)\n",
    "#   elif sentiment not in [\"positive\", \"negative\"]:\n",
    "#     # Confirm that the user will give an accurate value for the sentiment\n",
    "#     raise ValueError(\"Sentiment must be a string that is either positive or negative\") \n",
    "#   else:\n",
    "#     most_common_grams = get_top_k_ngrams(corpus=df[df['sentiment'] == sentiment].review,k=k, bi_only=bi_only)\n",
    "\n",
    "#   most_common_grams = dict(most_common_grams)\n",
    "#   ngrams = list(most_common_grams.keys())\n",
    "#   frequencies = list(most_common_grams.values())\n",
    "\n",
    "# #   colors = [\n",
    "# #     '#1f77b4',  # muted blue\n",
    "# #     '#ff7f0e',  # safety orange\n",
    "# #     '#2ca02c',  # cooked asparagus green\n",
    "# #     '#d62728',  # brick red\n",
    "# #     '#9467bd',  # muted purple\n",
    "# #     '#8c564b',  # chestnut brown\n",
    "# #     '#e377c2',  # raspberry yogurt pink\n",
    "# #     '#7f7f7f',  # middle gray\n",
    "# #     '#bcbd22',  # curry yellow-green\n",
    "# #     '#17becf'   # blue-teal\n",
    "# # ]\n",
    "\n",
    "#   bar_fig = go.Bar(y=frequencies, x=ngrams)\n",
    "#   return bar_fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "\n",
    "# df_gram_analysis = pd.DataFrame(list(zip(preprocessed_docs.doc_words, df['sentiment'])),\n",
    "#                columns =['review', 'sentiment'])\n",
    "\n",
    "# # Unigram Analysis\n",
    "# fig = make_subplots(rows=1, cols=2, shared_yaxes=True, subplot_titles=(\"Unigrams for positive reviews\", \"Unigrams for negative reviews\"))\n",
    "\n",
    "# for count, sentiment in enumerate([\"positive\", \"negative\"]):\n",
    "#   fig.add_trace(\n",
    "#       get_barchart_for_top_k_ngrams(sentiment=sentiment, x_axis_text=\"word\", df=df_gram_analysis, k=10, bi_only=True),\n",
    "#       row=1, col=count+1\n",
    "#       )\n",
    "\n",
    "# fig.update_layout(height=600, width=800, title_text=\"Most common unigrams sentiment wise\", showlegend=False)\n",
    "# fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build up Word Model - Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the text are properly processed, we're ready to train our word2vec via Gensim. Here I chose the dimension size 100 for each word embedding and window size of 5. The training iterates for 100 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cpu cores to work: 7\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "import sys\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "core_workers = multiprocessing.cpu_count() - 1 # Leave one core out\n",
    "print('Number of cpu cores to work: {}'.format(core_workers))\n",
    "assert gensim.models.doc2vec.FAST_VERSION > -1, \"This will be very slow otherwise.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with the word2vec_reviews model already exists\n"
     ]
    }
   ],
   "source": [
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after every epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "        self.start = timeit.default_timer()\n",
    "        self.stop = None\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        self.stop = timeit.default_timer()\n",
    "        print('Loss after epoch {}: {},  Epoch time: {} seconds'.format(self.epoch, loss_now, str(self.stop - self.start)))\n",
    "        self.epoch += 1\n",
    "        self.start = timeit.default_timer()\n",
    "\n",
    "\n",
    "if not os.path.exists('./' + WORD2VEC_FILE):\n",
    "    word_model = Word2Vec(preprocessed_docs.doc_words, min_count=2, vector_size=VECTOR_SIZE, window=5, workers=core_workers, epochs=175, \n",
    "                      compute_loss=True, callbacks=[callback()])\n",
    "else:\n",
    "    print(\"File with the word2vec_reviews model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model from file\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Saving or loading word2vec model to/from a file\n",
    "\n",
    "if os.path.exists('./' + WORD2VEC_FILE):\n",
    "    # Read the saved model\n",
    "    print('Reading model from file')\n",
    "    try:\n",
    "        word_model = gensim.models.Word2Vec.load(WORD2VEC_FILE)\n",
    "        print('Done')\n",
    "    except Exception as e:    \n",
    "        print('Could not load file', e)\n",
    "else:  \n",
    "    # Save word2vec model object as a model file.\n",
    "    print('Saving documents as a pickle')\n",
    "    try:\n",
    "        word_model.save(WORD2VEC_FILE)\n",
    "        print('Done')\n",
    "    except Exception as e:\n",
    "        print('Could not save file', e)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Averaging Word Embedding for Each Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK! Now we have the word embedding at hand, we'll be using the word embedding to compute for representative vector for whole text. It then serves as feature input for text classification model. There are various ways to come up with doc vector. First, let's start with the simple one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Averaging on Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a rather straightforward method. It directly averages all word embedding occurred in the text. Here I adapted the code from these two posts [2](http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/), [3](https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568) and created the python class **MeanWordEmbeddingVectorizer**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with the average word2vec model already exists\n"
     ]
    }
   ],
   "source": [
    "class MeanEmbeddingVectorizer(object):\n",
    "\n",
    "\tdef __init__(self, word_model):\n",
    "\t\tself.word_model = word_model\n",
    "\t\tself.vector_size = word_model.wv.vector_size\n",
    "\n",
    "\tdef fit(self):  # comply with scikit-learn transformer requirement\n",
    "\t\treturn self\n",
    "\n",
    "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
    "\t\tdoc_word_vector = self.word_average_list(docs)\n",
    "\t\treturn doc_word_vector\n",
    "\n",
    "\tdef word_average(self, sent):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute average word vector for a single doc/sentence.\n",
    "\t\t:param sent: list of sentence tokens\n",
    "\t\t:return:\n",
    "\t\t\tmean: float of averaging word vectors\n",
    "\t\t\"\"\"\n",
    "\t\tmean = []\n",
    "\t\tfor word in sent:\n",
    "\t\t\tif word in self.word_model.wv:\n",
    "\t\t\t\tmean.append(self.word_model.wv.get_vector(word))\n",
    "\n",
    "\t\tif not mean:  # empty words\n",
    "\t\t\t# If a text is empty, return a vector of zeros.\n",
    "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "\t\t\treturn np.zeros(self.vector_size)\n",
    "\t\telse:\n",
    "\t\t\tmean = np.array(mean).mean(axis=0)\n",
    "\t\t\treturn mean\n",
    "\n",
    "\n",
    "\tdef word_average_list(self, docs):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
    "\t\t:param docs: list of sentence in list of separated tokens\n",
    "\t\t:return:\n",
    "\t\t\tarray of average word vector in shape (len(docs),)\n",
    "\t\t\"\"\"\n",
    "\t\treturn np.vstack([self.word_average(sent) for sent in docs])\n",
    "    \n",
    "    \n",
    "if not os.path.exists('./' + MEAN_EMBEDDING_FILE):\n",
    "    mean_vec_tr = MeanEmbeddingVectorizer(word_model)\n",
    "    doc_vec = mean_vec_tr.transform(preprocessed_docs.doc_words)\n",
    "else:\n",
    "    print(\"File with the average word2vec model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model from file\n",
      "Done\n",
      "Shape of word-mean doc2vec: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49582, 200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving or loading averaging word2vec model to/from a file\n",
    "if os.path.exists('./' + MEAN_EMBEDDING_FILE):\n",
    "    # Read the saved model from a csv file\n",
    "    print('Reading model from file')\n",
    "    try:\n",
    "        doc_vec = genfromtxt(MEAN_EMBEDDING_FILE, delimiter=',')\n",
    "        print('Done')\n",
    "    except Exception as e:    \n",
    "        print('Could not load file', e)\n",
    "else:  \n",
    "    # Save model object as a csv file\n",
    "    print('Saving documents as a pickle')\n",
    "    try:\n",
    "        np.savetxt(os.path.join(MEAN_EMBEDDING_FILE), doc_vec, delimiter=',')\n",
    "        print('Done')\n",
    "    except Exception as e:\n",
    "        print('Could not save file', e)\n",
    "\n",
    "print('Shape of word-mean doc2vec: ')\n",
    "display(doc_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Weighted Averaging on Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not just satisfied with simple averaging? We can further adopt TF-IDF as weights for each word embedding. This will amplify the role of significant word in computing doc vector. Here, the whole process is implemented under class **TfidfEmbeddingVectorizer**. Again, the code is adapted from the same post source.\n",
    "\n",
    "One thing worth noted is that, the Term Frequency has already been considered when we conduct averaging over the text, but not Inverse Document Frequency, thus the weight literally being the IDF, and the unseen word is assigned the max IDF in default setting.\n",
    "\n",
    "And the other thing to note is that we need to fit the class with tokens first, for it must loop through all the words before hand in order to compute IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with the tf-idf word2vec model already exists\n"
     ]
    }
   ],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "\n",
    "\tdef __init__(self, word_model):\n",
    "\n",
    "\t\tself.word_model = word_model\n",
    "\t\tself.word_idf_weight = None\n",
    "\t\tself.vector_size = word_model.wv.vector_size\n",
    "\n",
    "\tdef fit(self, docs):  # comply with scikit-learn transformer requirement\n",
    "\t\t\"\"\"\n",
    "\t\tFit in a list of docs, which had been preprocessed and tokenized,\n",
    "\t\tsuch as word bi-grammed, stop-words removed, lemmatized, part of speech filtered.\n",
    "\t\tThen build up a tfidf model to compute each word's idf as its weight.\n",
    "\t\tNoted that tf weight is already involved when constructing average word vectors, and thus omitted.\n",
    "\t\t:param\n",
    "\t\t\tpre_processed_docs: list of docs, which are tokenized\n",
    "\t\t:return:\n",
    "\t\t\tself\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\ttext_docs = []\n",
    "\t\tfor doc in docs:\n",
    "\t\t\ttext_docs.append(\" \".join(doc))\n",
    "\n",
    "\t\ttfidf = TfidfVectorizer()\n",
    "\t\ttfidf.fit(text_docs)  # must be list of text string\n",
    "\n",
    "\t\t# if a word was never seen - it must be at least as infrequent\n",
    "\t\t# as any of the known words - so the default idf is the max of\n",
    "\t\t# known idf's\n",
    "\t\tmax_idf = max(tfidf.idf_)  # used as default value for defaultdict\n",
    "\t\tself.word_idf_weight = defaultdict(lambda: max_idf,\n",
    "\t\t\t\t\t\t\t\t\t\t   [(word, tfidf.idf_[i]) for word, i in tfidf.vocabulary_.items()])\n",
    "\t\treturn self\n",
    "\n",
    "\n",
    "\tdef transform(self, docs):  # comply with scikit-learn transformer requirement\n",
    "\t\tdoc_word_vector = self.word_average_list(docs)\n",
    "\t\treturn doc_word_vector\n",
    "\n",
    "\n",
    "\tdef word_average(self, sent):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute average word vector for a single doc/sentence.\n",
    "\t\t:param sent: list of sentence tokens\n",
    "\t\t:return:\n",
    "\t\t\tmean: float of averaging word vectors\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\tmean = []\n",
    "\t\tfor word in sent:\n",
    "\t\t\tif word in self.word_model.wv.index_to_key:\n",
    "\t\t\t\tmean.append(self.word_model.wv.get_vector(word) * self.word_idf_weight[word])  # idf weighted\n",
    "\n",
    "\t\tif not mean:  # empty words\n",
    "\t\t\t# If a text is empty, return a vector of zeros.\n",
    "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "\t\t\treturn np.zeros(self.vector_size)\n",
    "\t\telse:\n",
    "\t\t\tmean = np.array(mean).mean(axis=0)\n",
    "\t\t\treturn mean\n",
    "\n",
    "\n",
    "\tdef word_average_list(self, docs):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute average word vector for multiple docs, where docs had been tokenized.\n",
    "\t\t:param docs: list of sentence in list of separated tokens\n",
    "\t\t:return:\n",
    "\t\t\tarray of average word vector in shape (len(docs),)\n",
    "\t\t\"\"\"\n",
    "\t\treturn np.vstack([self.word_average(sent) for sent in docs])\n",
    "\n",
    "if not os.path.exists('./' + TFIDF_EMBEDDING_FILE):\n",
    "    tfidf_vec_tr = TfidfEmbeddingVectorizer(word_model)\n",
    "    tfidf_vec_tr.fit(preprocessed_docs.doc_words)  # fit tfidf model first\n",
    "    tfidf_doc_vec = tfidf_vec_tr.transform(preprocessed_docs.doc_words)\n",
    "else:\n",
    "    print(\"File with the tf-idf word2vec model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model from file\n",
      "Done\n",
      "Shape of tf-idf doc2vec: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49582, 200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving or loading tf-idf word2vec model to/from a file\n",
    "if os.path.exists('./' + TFIDF_EMBEDDING_FILE):\n",
    "    # Read the saved model from a csv file\n",
    "    print('Reading model from file')\n",
    "    try:\n",
    "        tfidf_doc_vec = genfromtxt(TFIDF_EMBEDDING_FILE, delimiter=',')\n",
    "        print('Done')\n",
    "    except Exception as e:    \n",
    "        print('Could not load file', e)\n",
    "else:  \n",
    "    # Save model object as a csv file\n",
    "    print('Saving documents as a pickle')\n",
    "    try:\n",
    "        np.savetxt(os.path.join(TFIDF_EMBEDDING_FILE), tfidf_doc_vec, delimiter=',')\n",
    "        print('Done')\n",
    "    except Exception as e:\n",
    "        print('Could not save file', e)\n",
    "\n",
    "print('Shape of tf-idf doc2vec: ')\n",
    "display(tfidf_doc_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leverage Pre-train GloVe Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in GloVe Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include another option-leveraging the existing pre-trained word embedding and see how it performs in text classification. Here I follow up the instructions from [Stanford NLP course(CS224N) notebook](http://web.stanford.edu/class/cs224n/materials/Gensim%20word%20vector%20visualization.html), importing GloVe word embedding into Gensim to compute for averaging word embedding on text.\n",
    "\n",
    "As a side note, I've also tried to apply Tf-IDF weighted method on GloVe vector, but found out that the result is basically the same as the ones from TF-IDF weighted averaging doc vector. Thus, I omit the demonstration and just include simple averaging on GloVe word vector here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**APPENDIX: The explanation for logic behind loading pre-train word vector.**</mark>\n",
    "\n",
    "The result of `datapath()` shows that Gensim will try to load in dataset from */Users/XXX/miniconda3/lib/python3.7/site-packages/gensim/test/test_data/*, and calls it `glove_vec_fi`.\n",
    "\n",
    "It then uses `get_tmpfile()` to create a temporary file path to store the word2vec `tmp_word2vec_fi`, which is converted from `glove_vec_fi`.\n",
    "\n",
    "At the final step, **KeyedVectors** then loads in the `tmp_word2vec_fi` as word model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Averaging on GloVe Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with the average word2vec GloVe model already exists\n"
     ]
    }
   ],
   "source": [
    "class MeanEmbeddingVectorizerGlove(MeanEmbeddingVectorizer):\n",
    "\n",
    "\tdef __init__(self, word_model):\n",
    "\t\tself.word_model = word_model\n",
    "\t\tself.vector_size = word_model.vector_size\n",
    "\n",
    "\tdef word_average(self, sent):\n",
    "\t\t\"\"\"\n",
    "\t\tCompute average word vector for a single doc/sentence.\n",
    "\t\t:param sent: list of sentence tokens\n",
    "\t\t:return:\n",
    "\t\t\tmean: float of averaging word vectors\n",
    "\t\t\"\"\"\n",
    "\t\tmean = []\n",
    "\t\tfor word in sent:\n",
    "\t\t\tif word in self.word_model:\n",
    "\t\t\t\tmean.append(self.word_model.get_vector(word))\n",
    "\n",
    "\t\tif not mean:  # empty words\n",
    "\t\t\t# If a text is empty, return a vector of zeros.\n",
    "\t\t\tlogging.warning(\"cannot compute average owing to no vector for {}\".format(sent))\n",
    "\t\t\treturn np.zeros(self.vector_size)\n",
    "\t\telse:\n",
    "\t\t\tmean = np.array(mean).mean(axis=0)\n",
    "\t\t\treturn mean\n",
    "\n",
    "\n",
    "# Load in GloVe vector.\n",
    "if not os.path.exists('./' + GLOVE_EMBEDDING_FILE):\n",
    "    glove_vec_fi = datapath('/home/nick/PycharmProjects/pythonProject/Homework/glove_emb/glove.6B.200d.txt')\n",
    "    tmp_word2vec_fi = get_tmpfile('tmp_glove2word2vec.txt')\n",
    "\n",
    "    glove2word2vec(glove_vec_fi, tmp_word2vec_fi)\n",
    "\n",
    "    glove_word_model = KeyedVectors.load_word2vec_format(tmp_word2vec_fi)\n",
    "\n",
    "    # Apply word averaging on GloVe word vector.\n",
    "    glove_mean_vec_tr = MeanEmbeddingVectorizerGlove(glove_word_model)\n",
    "    glove_doc_vec = glove_mean_vec_tr.transform(preprocessed_docs.doc_words)\n",
    "else:\n",
    "    print(\"File with the average word2vec GloVe model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model from file\n",
      "Done\n",
      "Shape of mean glove_doc_vec: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49582, 200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving or loading averaging glove model to/from a file\n",
    "if os.path.exists('./' + GLOVE_EMBEDDING_FILE):\n",
    "    # Read the saved model from a csv file\n",
    "    print('Reading model from file')\n",
    "    try:\n",
    "        glove_doc_vec = genfromtxt(GLOVE_EMBEDDING_FILE, delimiter=',')\n",
    "        print('Done')\n",
    "    except Exception as e:    \n",
    "        print('Could not load file', e)\n",
    "else:  \n",
    "    # Save model object as a csv file\n",
    "    print('Saving documents as a csv')\n",
    "    try:\n",
    "        np.savetxt(os.path.join(GLOVE_EMBEDDING_FILE), glove_doc_vec, delimiter=',')\n",
    "        print('Done')\n",
    "    except Exception as e:\n",
    "        print('Could not save file', e)\n",
    "\n",
    "print('Shape of mean glove_doc_vec: ')\n",
    "display(glove_doc_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Examine if glove_doc_vec is equal to self-trained doc_vec...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Examine if glove_doc_vec is equal to self-trained doc_vec.\n",
    "print('Examine if glove_doc_vec is equal to self-trained doc_vec...')\n",
    "glove_doc_vec[4] == doc_vec[4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Doc2vec Training Directly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV-DM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last but not least, we still have one more option-to directly train doc2vec, and no need to average all word embeddings. Here I chose **PV-DM model** to train my doc2vec.\n",
    "\n",
    "The script is mostly referred from [Gensim tutorial](https://github.com/RaRe-Technologies/gensim/blob/3c3506d51a2caf6b890de3b1b32a8b85f7566ca5/docs/notebooks/doc2vec-IMDB.ipynb). And again, to save all the labor, I create a class **DocModel** for it. The class just needs to take in the **TaggedDocument** and then we call `self.custom_train()` method, the doc model will train itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**APPENDIX: Training with Fixed Learning Rate.**</mark>\n",
    "\n",
    "It's said to achieve better result, but the statement is from a rather old-version gensim tutorial. I found no better training result out of using fixed learning rate. Instead, using the default one, which is also recommended by the new gensim document achieving better performance.\n",
    "\n",
    "Excerpted from [Doc2vec tutorial](https://rare-technologies.com/doc2vec-tutorial/)\n",
    "\n",
    "I have obtained better results by iterating over the data several times and either\n",
    "\n",
    "1. randomizing the order of input sentences, or\n",
    "2. manually controlling the learning rate over the course of several iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocModel(object):\n",
    "\n",
    "\tdef __init__(self, docs, **kwargs):\n",
    "\t\t\"\"\"\n",
    "\t\t:param docs: list of TaggedDocument\n",
    "\t\t:param kwargs: dictionary of (key,value) for Doc2Vec arguments\n",
    "\t\t\"\"\"\n",
    "\t\tself.model = Doc2Vec(**kwargs)\n",
    "\t\tself.docs = docs\n",
    "\t\tself.model.build_vocab([x for x in self.docs])\n",
    "\n",
    "\tdef custom_train(self, fixed_lr=False, fixed_lr_epochs=None):\n",
    "\t\t\"\"\"\n",
    "\t\tTrain Doc2Vec with two options, without fixed learning rate(recommended) or with fixed learning rate.\n",
    "\t\tFixed learning rate also includes implementation of shuffling training dataset.\n",
    "\t\t:param fixed_lr: boolean\n",
    "\t\t:param fixed_lr_epochs: num of epochs for fixed lr training\n",
    "\t\t\"\"\"\n",
    "\t\tif not fixed_lr:\n",
    "\t\t\tself.model.train([x for x in self.docs],\n",
    "\t\t\t\t\t\t\t total_examples=len(self.docs),\n",
    "\t\t\t\t\t\t\t epochs=self.model.epochs)\n",
    "\t\telse:\n",
    "\t\t\tfor _ in range(fixed_lr_epochs):\n",
    "\t\t\t\tself.model.train(utils.shuffle([x for x in self.docs]),\n",
    "\t\t\t\t\t\t\t\t total_examples=len(self.docs),\n",
    "\t\t\t\t\t\t\t\t epochs=1)\n",
    "\t\t\t\tself.model.alpha -= 0.002\n",
    "\t\t\t\tself.model.min_alpha = self.model.alpha  # fixed learning rate\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with the doc2vec model already exists\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('./' + DOC2VEC_EMBEDDING_FILE):\n",
    "\n",
    "    # Instantiate a pv-dm model\n",
    "    # Configure keyed arguments for Doc2Vec model.\n",
    "    dm_args = {\n",
    "        'dm': 1,\n",
    "        'dm_mean': 1,\n",
    "        'vector_size': 200, # VECTOR_SIZE IMPORTANT!!!\n",
    "        'window': 5,\n",
    "        'negative': 5,\n",
    "        'hs': 0,\n",
    "        'min_count': 2,\n",
    "        'sample': 0,\n",
    "        'workers': 7-1,\n",
    "        'alpha': 0.025,\n",
    "        'min_alpha': 0.025,\n",
    "        'epochs': 175,\n",
    "        'comment': 'alpha=0.025'\n",
    "    }\n",
    "    dm = DocModel(docs=preprocessed_docs.tagdocs, **dm_args)\n",
    "    dm.custom_train()\n",
    "\n",
    "    # Save doc2vec as feature dataframe.\n",
    "    dm_doc_vec_ls = []\n",
    "    for i in range(len(dm.model.docvecs)):\n",
    "        dm_doc_vec_ls.append(dm.model.docvecs[i])\n",
    "\n",
    "    dm_doc_vec = pd.DataFrame(dm_doc_vec_ls)\n",
    "else:\n",
    "    print(\"File with the doc2vec model already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading doc2vec model from file\n",
      "Done\n",
      "Shape of doc2vec: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(49582, 200)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Saving or loading doc2vec model to/from a file\n",
    "if os.path.exists('./' + DOC2VEC_EMBEDDING_FILE):\n",
    "    # Read the saved model from a csv file\n",
    "    print('Reading doc2vec model from file')\n",
    "    try:\n",
    "        dm_doc_vec = pd.read_csv(os.path.join(DOC2VEC_EMBEDDING_FILE), header=None)\n",
    "        print('Done')\n",
    "    except Exception as e:    \n",
    "        print('Could not load file', e)\n",
    "else:  \n",
    "    # Save model object as a csv file\n",
    "    print('Saving documents as a csv')\n",
    "    try:\n",
    "        dm_doc_vec.to_csv(os.path.join(DOC2VEC_EMBEDDING_FILE), index=False, header=False)\n",
    "        print('Done')\n",
    "    except Exception as e:\n",
    "        print('Could not save file', e)\n",
    "\n",
    "print('Shape of doc2vec: ')\n",
    "display(dm_doc_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Classification Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we've prepared all the necessary ingredients-different types of features. Let's experiment to observe their effect on classification performance. Here, I'll use **basic logistic model** as the base model and feed in different kind of features created earlier. Hence, to compare their effectiveness.\n",
    "\n",
    "In addition to compare effects of each word embedding averaging method, I also try to **concatenate word2vec and doc2vec** together, and see if it can boost up the performance even more.\n",
    "\n",
    "I used TF-IDF weighted word embedding and PV-DM doc2vec together. The result shows that it increases the accuracy on training dataset (perhaps a sign of over-fitting?), but not so significant improvement on testing dataset compared using TF-IDF word2vec alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get target_labels\n",
    "target_labels = preprocessed_docs.labels\n",
    "\n",
    "doc_vec = pd.read_csv(os.path.join('doc_vec_new.csv'), header=None)\n",
    "tfidf_doc_vec = pd.read_csv(os.path.join('tfidf_doc_vec_new.csv'), header=None)\n",
    "glove_doc_vec = pd.read_csv(os.path.join('glove_doc_vec_new.csv'), header=None)\n",
    "dm_doc_vec = pd.read_csv(os.path.join('dm_doc_vec_new.csv'), header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification via Logistic Model\n",
    "# logistic = LogisticRegression(random_state=random_state, multi_class='multinomial', solver='saga')\n",
    "logistic = LogisticRegression(penalty='l2', max_iter=500, random_state=random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Data Split Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_data_splits(df, concat_dfs=None):\n",
    "    if concat_dfs is not None:\n",
    "        for concat_df in concat_dfs:\n",
    "            df = pd.concat([df, concat_df], axis=1, ignore_index=True)\n",
    "    \n",
    "    # Prepare test dataset\n",
    "    train_X, test_X, train_y, test_y = train_test_split(df,\n",
    "                                                    target_labels,\n",
    "                                                    test_size=TEST_SIZE,\n",
    "                                                    random_state=random_state,\n",
    "                                                    stratify=target_labels)\n",
    "    # Prepare valid dataset\n",
    "    if VALID_SIZE != 0:\n",
    "        train_X, valid_X, train_y, valid_y = train_test_split(train_X,\n",
    "                                                      train_y,\n",
    "                                                      test_size=VALID_SIZE,\n",
    "                                                      random_state=random_state,\n",
    "                                                      stratify=train_y)\n",
    "    \n",
    "    print('Shape of train_X: {}'.format(train_X.shape))\n",
    "    print('Shape of valid_X: {}'.format(valid_X.shape if 'valid_X' in vars() else (0,0)))\n",
    "    print('Shape of text_X: {}'.format(test_X.shape))\n",
    "    \n",
    "    if VALID_SIZE != 0:\n",
    "        return train_X, valid_X, test_X, train_y, valid_y, test_y\n",
    "    else:\n",
    "        return train_X, None, test_X, train_y, None, test_y\n",
    "    \n",
    "def evaluate_model(model, feature, label, label_names):\n",
    "\tpred = model.predict(feature)\n",
    "\ttrue = np.array(label)\n",
    "\n",
    "\tprint('Score on dataset...\\n')\n",
    "\tprint('Confusion Matrix:\\n', confusion_matrix(true, pred))\n",
    "\tprint('\\nClassification Report:\\n', classification_report(true, pred, target_names=label_names))\n",
    "\tprint('\\naccuracy: {:.3f}'.format(accuracy_score(true, pred)))\n",
    "\tprint('f1 score: {:.3f}'.format(f1_score(true, pred, average='weighted')))\n",
    "\n",
    "\treturn pred, true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Simple Averaging Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logistic  # or choose sgd.\n",
    "df = doc_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (39665, 200)\n",
      "Shape of valid_X: (0, 0)\n",
      "Shape of text_X: (9917, 200)\n",
      "Training time2.6209180309961084 seconds\n"
     ]
    }
   ],
   "source": [
    "# __init_model_data_splits__\n",
    "train_X, valid_X, test_X, train_y, valid_y, test_y = init_data_splits(df)\n",
    "\n",
    "start = timeit.default_timer()\n",
    "clf = model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time{} seconds'.format(str(stop - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score on valid dataset.\n",
    "print('Performance of Mean Word Vector on training dataset...')\n",
    "_, _ = evaluate_model(clf, train_X, train_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Mean Word Vector on testing dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4319  621]\n",
      " [ 524 4453]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.87      0.88      4940\n",
      "    positive       0.88      0.89      0.89      4977\n",
      "\n",
      "    accuracy                           0.88      9917\n",
      "   macro avg       0.88      0.88      0.88      9917\n",
      "weighted avg       0.88      0.88      0.88      9917\n",
      "\n",
      "\n",
      "accuracy: 0.885\n",
      "f1 score: 0.885\n"
     ]
    }
   ],
   "source": [
    "# Score on test dataset.\n",
    "print('Performance of Mean Word Vector on testing dataset...')\n",
    "_, _ = evaluate_model(clf, test_X, test_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Tf-Idf Weighted Averaging Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logistic  # or choose sgd.\n",
    "df = tfidf_doc_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (39665, 200)\n",
      "Shape of valid_X: (0, 0)\n",
      "Shape of text_X: (9917, 200)\n",
      "Training time1.7606628479989013 seconds\n"
     ]
    }
   ],
   "source": [
    "# __init_model_data_splits__\n",
    "train_X, valid_X, test_X, train_y, valid_y, test_y = init_data_splits(df)\n",
    "\n",
    "# Train model\n",
    "start = timeit.default_timer()\n",
    "clf = model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time{} seconds'.format(str(stop - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Tf-Idf Mean Word Vector on training dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[17432  2326]\n",
      " [ 2329 17578]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.88      0.88     19758\n",
      "    positive       0.88      0.88      0.88     19907\n",
      "\n",
      "    accuracy                           0.88     39665\n",
      "   macro avg       0.88      0.88      0.88     39665\n",
      "weighted avg       0.88      0.88      0.88     39665\n",
      "\n",
      "\n",
      "accuracy: 0.883\n",
      "f1 score: 0.883\n"
     ]
    }
   ],
   "source": [
    "# Score on valid dataset.\n",
    "print('Performance of Tf-Idf Mean Word Vector on training dataset...')\n",
    "_, _ = evaluate_model(clf, train_X, train_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Tf-Idf Mean Word Vector on testing dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4300  640]\n",
      " [ 542 4435]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.87      0.88      4940\n",
      "    positive       0.87      0.89      0.88      4977\n",
      "\n",
      "    accuracy                           0.88      9917\n",
      "   macro avg       0.88      0.88      0.88      9917\n",
      "weighted avg       0.88      0.88      0.88      9917\n",
      "\n",
      "\n",
      "accuracy: 0.881\n",
      "f1 score: 0.881\n"
     ]
    }
   ],
   "source": [
    "# Score on test dataset.\n",
    "print('Performance of Tf-Idf Mean Word Vector on testing dataset...')\n",
    "_, _ = evaluate_model(clf, test_X, test_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Simple Averaging of GloVe Word Vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logistic  # or choose sgd.\n",
    "df = glove_doc_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (39665, 200)\n",
      "Shape of valid_X: (0, 0)\n",
      "Shape of text_X: (9917, 200)\n",
      "Training time4.669959749997361 seconds\n"
     ]
    }
   ],
   "source": [
    "# __init_model_data_splits__\n",
    "train_X, valid_X, test_X, train_y, valid_y, test_y = init_data_splits(df)\n",
    "\n",
    "# Train model\n",
    "start = timeit.default_timer()\n",
    "clf = model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time{} seconds'.format(str(stop - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of GloVe Mean Word Vector on training dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[16102  3656]\n",
      " [ 3552 16355]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.81      0.82     19758\n",
      "    positive       0.82      0.82      0.82     19907\n",
      "\n",
      "    accuracy                           0.82     39665\n",
      "   macro avg       0.82      0.82      0.82     39665\n",
      "weighted avg       0.82      0.82      0.82     39665\n",
      "\n",
      "\n",
      "accuracy: 0.818\n",
      "f1 score: 0.818\n"
     ]
    }
   ],
   "source": [
    "# Score on valid dataset.\n",
    "print('Performance of GloVe Mean Word Vector on training dataset...')\n",
    "_, _ = evaluate_model(clf, train_X, train_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of GloVe Mean Word Vector on testing dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3971  969]\n",
      " [ 871 4106]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.80      0.81      4940\n",
      "    positive       0.81      0.82      0.82      4977\n",
      "\n",
      "    accuracy                           0.81      9917\n",
      "   macro avg       0.81      0.81      0.81      9917\n",
      "weighted avg       0.81      0.81      0.81      9917\n",
      "\n",
      "\n",
      "accuracy: 0.814\n",
      "f1 score: 0.814\n"
     ]
    }
   ],
   "source": [
    "# Score on test dataset.\n",
    "print('Performance of GloVe Mean Word Vector on testing dataset...')\n",
    "_, _ = evaluate_model(clf, test_X, test_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on PV-DM Doc2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logistic  # or choose sgd.\n",
    "df = dm_doc_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (39665, 200)\n",
      "Shape of valid_X: (0, 0)\n",
      "Shape of text_X: (9917, 200)\n",
      "Training time0.7849324400012847 seconds\n"
     ]
    }
   ],
   "source": [
    "# __init_model_data_splits__\n",
    "train_X, valid_X, test_X, train_y, valid_y, test_y = init_data_splits(df)\n",
    "\n",
    "# Train model\n",
    "start = timeit.default_timer()\n",
    "clf = model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time{} seconds'.format(str(stop - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Doc2vec on training dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[17161  2597]\n",
      " [ 2580 17327]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.87      0.87     19758\n",
      "    positive       0.87      0.87      0.87     19907\n",
      "\n",
      "    accuracy                           0.87     39665\n",
      "   macro avg       0.87      0.87      0.87     39665\n",
      "weighted avg       0.87      0.87      0.87     39665\n",
      "\n",
      "\n",
      "accuracy: 0.869\n",
      "f1 score: 0.869\n"
     ]
    }
   ],
   "source": [
    "# Score on valid dataset.\n",
    "print('Performance of Doc2vec on training dataset...')\n",
    "_, _ = evaluate_model(clf, train_X, train_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Doc2vec on testing dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4270  670]\n",
      " [ 655 4322]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.87      0.86      0.87      4940\n",
      "    positive       0.87      0.87      0.87      4977\n",
      "\n",
      "    accuracy                           0.87      9917\n",
      "   macro avg       0.87      0.87      0.87      9917\n",
      "weighted avg       0.87      0.87      0.87      9917\n",
      "\n",
      "\n",
      "accuracy: 0.866\n",
      "f1 score: 0.866\n"
     ]
    }
   ],
   "source": [
    "# Score on test dataset.\n",
    "print('Performance of Doc2vec on testing dataset...')\n",
    "_, _ = evaluate_model(clf, test_X, test_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Tf-Idf and Doc2vec Concatenated Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = logistic  # or choose sgd.\n",
    "df = tfidf_doc_vec\n",
    "concat_dfs = [dm_doc_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (39665, 400)\n",
      "Shape of valid_X: (0, 0)\n",
      "Shape of text_X: (9917, 400)\n",
      "Training time4.565859313996043 seconds\n"
     ]
    }
   ],
   "source": [
    "train_X, valid_X, test_X, train_y, valid_y, test_y = init_data_splits(df, concat_dfs=concat_dfs)\n",
    "\n",
    "# Train model\n",
    "start = timeit.default_timer()\n",
    "clf = model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time{} seconds'.format(str(stop - start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Tf-Idf+Doc2vec on training dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[17698  2060]\n",
      " [ 2056 17851]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.90      0.90     19758\n",
      "    positive       0.90      0.90      0.90     19907\n",
      "\n",
      "    accuracy                           0.90     39665\n",
      "   macro avg       0.90      0.90      0.90     39665\n",
      "weighted avg       0.90      0.90      0.90     39665\n",
      "\n",
      "\n",
      "accuracy: 0.896\n",
      "f1 score: 0.896\n"
     ]
    }
   ],
   "source": [
    "# Score on valid dataset.\n",
    "print('Performance of Tf-Idf+Doc2vec on training dataset...')\n",
    "_, _ = evaluate_model(clf, train_X, train_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance of Tf-Idf+Doc2vec on testing dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4368  572]\n",
      " [ 476 4501]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89      4940\n",
      "    positive       0.89      0.90      0.90      4977\n",
      "\n",
      "    accuracy                           0.89      9917\n",
      "   macro avg       0.89      0.89      0.89      9917\n",
      "weighted avg       0.89      0.89      0.89      9917\n",
      "\n",
      "\n",
      "accuracy: 0.894\n",
      "f1 score: 0.894\n"
     ]
    }
   ],
   "source": [
    "# Score on test dataset.\n",
    "print('Performance of Tf-Idf+Doc2vec on testing dataset...')\n",
    "_, _ = evaluate_model(clf, test_X, test_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance on Tf-Idf  + Simple Averaging of GloVe Word Vector + Doc2vec Concatenated Feature???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (39665, 600)\n",
      "Shape of valid_X: (0, 0)\n",
      "Shape of text_X: (9917, 600)\n",
      "Training time24.04210009900271 seconds\n",
      "Performance of Tf-Idf+Doc2vec on training dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[17724  2034]\n",
      " [ 2015 17892]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.90      0.90     19758\n",
      "    positive       0.90      0.90      0.90     19907\n",
      "\n",
      "    accuracy                           0.90     39665\n",
      "   macro avg       0.90      0.90      0.90     39665\n",
      "weighted avg       0.90      0.90      0.90     39665\n",
      "\n",
      "\n",
      "accuracy: 0.898\n",
      "f1 score: 0.898\n",
      "Performance of Tf-Idf+Doc2vec on testing dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4357  583]\n",
      " [ 486 4491]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.88      0.89      4940\n",
      "    positive       0.89      0.90      0.89      4977\n",
      "\n",
      "    accuracy                           0.89      9917\n",
      "   macro avg       0.89      0.89      0.89      9917\n",
      "weighted avg       0.89      0.89      0.89      9917\n",
      "\n",
      "\n",
      "accuracy: 0.892\n",
      "f1 score: 0.892\n"
     ]
    }
   ],
   "source": [
    "model = logistic  # or choose sgd.\n",
    "df = tfidf_doc_vec\n",
    "concat_dfs = [dm_doc_vec, glove_doc_vec]\n",
    "\n",
    "train_X, valid_X, test_X, train_y, valid_y, test_y = init_data_splits(df, concat_dfs=concat_dfs)\n",
    "\n",
    "# Train model\n",
    "start = timeit.default_timer()\n",
    "clf = model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time{} seconds'.format(str(stop - start)))\n",
    "\n",
    "# Score on train dataset.\n",
    "print('Performance of Tf-Idf+Doc2vec on training dataset...')\n",
    "_, _ = evaluate_model(clf, train_X, train_y, label_names=None)\n",
    "\n",
    "# Score on test dataset.\n",
    "print('Performance of Tf-Idf+Doc2vec on testing dataset...')\n",
    "_, _ = evaluate_model(clf, test_X, test_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-Search for Best Hyper-Paremeters for the tfidf + doc2vec approach\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_X: (39665, 400)\n",
      "Shape of valid_X: (0, 0)\n",
      "Shape of text_X: (9917, 400)\n"
     ]
    }
   ],
   "source": [
    "# Using the tfidf + doc_vec approach\n",
    "\n",
    "df = tfidf_doc_vec\n",
    "concat_dfs = [dm_doc_vec]\n",
    "train_X, valid_X, test_X, train_y, valid_y, test_y = init_data_splits(df, concat_dfs=concat_dfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid-Search for Best Hyper-Paremeters on Logistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**NOTE:**</mark>\n",
    "\n",
    "The `log_loss()` from sklearn expects to have predicted probs for each class. Thus, instead of simply using `clf.predict(X)`, we need to use `clf.predict_proba(X)` when computing for y_pred. In other words, if we want to use `GridSearchCV`, it's probabily best to directly specify scoring as **'neg_log_loss'**, instead of creating our own scoring func."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 3 candidates, totalling 30 fits\n",
      "[CV 1/10] END ..............................C=1;, score=0.891 total time=   4.9s\n",
      "[CV 2/10] END ..............................C=1;, score=0.891 total time=   4.2s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106822/2404523756.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mgrid_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1587\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m             \u001b[0mprefer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"processes\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m         fold_coefs_ = Parallel(\n\u001b[0m\u001b[1;32m   1590\u001b[0m             \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1039\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1041\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_regression_path\u001b[0;34m(X, y, pos_class, Cs, fit_intercept, max_iter, tol, verbose, solver, coef, class_weight, dual, penalty, intercept_scaling, multi_class, random_state, check_input, max_squared_sum, sample_weight, l1_ratio)\u001b[0m\n\u001b[1;32m    804\u001b[0m                 \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearchsorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m             ]\n\u001b[0;32m--> 806\u001b[0;31m             opt_res = optimize.minimize(\n\u001b[0m\u001b[1;32m    807\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m                 \u001b[0mw0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/scipy/optimize/_minimize.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[1;32m    621\u001b[0m                                   **options)\n\u001b[1;32m    622\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l-bfgs-b'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 623\u001b[0;31m         return _minimize_lbfgsb(fun, x0, args, jac, bounds,\n\u001b[0m\u001b[1;32m    624\u001b[0m                                 callback=callback, **options)\n\u001b[1;32m    625\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tnc'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/scipy/optimize/lbfgsb.py\u001b[0m in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    358\u001b[0m             \u001b[0;31m# until the completion of the current minimization iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0;31m# Overwrite f and g:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 360\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    361\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'NEW_X'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;31m# new iteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_x_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36m_update_fun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_update_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mupdate_fun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfun_wrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_fun_impl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/scipy/optimize/_differentiable_functions.py\u001b[0m in \u001b[0;36mfun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    132\u001b[0m             \u001b[0;31m# Overwriting results in undefined behaviour because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# fun(self.x) will change self.x, with the two no longer linked.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mupdate_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;34m\"\"\" returns the the function value \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_if_needed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/scipy/optimize/optimize.py\u001b[0m in \u001b[0;36m_compute_if_needed\u001b[0;34m(self, x, *args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m             \u001b[0mfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36m_logistic_loss_and_grad\u001b[0;34m(w, X, y, alpha, sample_weight)\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0mz0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0malpha\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Case where we fit the intercept.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(penalty='l2', max_iter=500, random_state=random_state)\n",
    "params_log = {\"C\": [1, 2, 5]}\n",
    "\n",
    "grid_log = GridSearchCV(model, param_grid=params_log, scoring='accuracy', n_jobs=core_workers-1, cv=5, verbose=4)\n",
    "\n",
    "# Train model\n",
    "start = timeit.default_timer()\n",
    "grid_log.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time {} seconds'.format(str(stop - start)))\n",
    "print(\"The best Accuracy of all model parameters' combination on model: {:.4f}\".format(grid_log.best_score_))\n",
    "\n",
    "display(pd.DataFrame(grid_log.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_train_score', 'mean_test_score', 'rank_test_score']])\n",
    "\n",
    "# Retrain on best model\n",
    "best_model = grid_log.best_estimator_\n",
    "start = timeit.default_timer()\n",
    "best_model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time {} seconds'.format(str(stop - start)))\n",
    "\n",
    "# Score on train dataset.\n",
    "print('Performance of optimized model on training dataset...')\n",
    "_, _ = evaluate_model(best_model, train_X, train_y, label_names=None)\n",
    "\n",
    "# Score on test dataset.\n",
    "print(\"the accuracy of the optimized model on testing dataset...\")\n",
    "_, _ = evaluate_model(best_model, test_X, test_y, label_names=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-Search for Best Hyper-Parameters on K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n",
      "[CV 1/10] END ....................n_neighbors=3;, score=0.771 total time=   4.5s\n",
      "[CV 2/10] END ....................n_neighbors=3;, score=0.772 total time=   4.4s\n",
      "[CV 3/10] END ....................n_neighbors=3;, score=0.785 total time=   4.8s\n",
      "[CV 4/10] END ....................n_neighbors=3;, score=0.752 total time=   7.1s\n",
      "[CV 5/10] END ....................n_neighbors=3;, score=0.760 total time=   6.4s\n",
      "[CV 6/10] END ....................n_neighbors=3;, score=0.774 total time=   9.5s\n",
      "[CV 7/10] END ....................n_neighbors=3;, score=0.766 total time=   5.2s\n",
      "[CV 8/10] END ....................n_neighbors=3;, score=0.756 total time=   5.5s\n",
      "[CV 9/10] END ....................n_neighbors=3;, score=0.770 total time=   4.5s\n",
      "[CV 10/10] END ...................n_neighbors=3;, score=0.770 total time=   4.9s\n",
      "[CV 1/10] END ....................n_neighbors=5;, score=0.791 total time=   5.3s\n",
      "[CV 2/10] END ....................n_neighbors=5;, score=0.787 total time=   5.5s\n",
      "[CV 3/10] END ....................n_neighbors=5;, score=0.810 total time=   6.2s\n",
      "[CV 4/10] END ....................n_neighbors=5;, score=0.780 total time=   5.2s\n",
      "[CV 5/10] END ....................n_neighbors=5;, score=0.780 total time=   4.8s\n",
      "[CV 6/10] END ....................n_neighbors=5;, score=0.798 total time=   5.4s\n",
      "[CV 7/10] END ....................n_neighbors=5;, score=0.784 total time=   4.9s\n",
      "[CV 8/10] END ....................n_neighbors=5;, score=0.781 total time=   5.3s\n",
      "[CV 9/10] END ....................n_neighbors=5;, score=0.795 total time=   5.0s\n",
      "[CV 10/10] END ...................n_neighbors=5;, score=0.788 total time=   4.9s\n",
      "[CV 1/10] END ...................n_neighbors=10;, score=0.828 total time=   5.3s\n",
      "[CV 2/10] END ...................n_neighbors=10;, score=0.825 total time=   5.6s\n",
      "[CV 3/10] END ...................n_neighbors=10;, score=0.844 total time=   5.6s\n",
      "[CV 4/10] END ...................n_neighbors=10;, score=0.810 total time=   5.5s\n",
      "[CV 5/10] END ...................n_neighbors=10;, score=0.817 total time=   5.9s\n",
      "[CV 6/10] END ...................n_neighbors=10;, score=0.830 total time=   5.6s\n",
      "[CV 7/10] END ...................n_neighbors=10;, score=0.828 total time=   7.7s\n",
      "[CV 8/10] END ...................n_neighbors=10;, score=0.825 total time=   7.0s\n",
      "[CV 9/10] END ...................n_neighbors=10;, score=0.824 total time=   6.9s\n",
      "[CV 10/10] END ..................n_neighbors=10;, score=0.827 total time=   5.8s\n",
      "[CV 1/10] END ...................n_neighbors=20;, score=0.838 total time=   7.4s\n",
      "[CV 2/10] END ...................n_neighbors=20;, score=0.837 total time=   6.7s\n",
      "[CV 3/10] END ...................n_neighbors=20;, score=0.852 total time=   6.7s\n",
      "[CV 4/10] END ...................n_neighbors=20;, score=0.832 total time=   6.2s\n",
      "[CV 5/10] END ...................n_neighbors=20;, score=0.838 total time=   7.0s\n",
      "[CV 6/10] END ...................n_neighbors=20;, score=0.841 total time=   6.5s\n",
      "[CV 7/10] END ...................n_neighbors=20;, score=0.844 total time=   4.8s\n",
      "[CV 8/10] END ...................n_neighbors=20;, score=0.845 total time=   5.0s\n",
      "[CV 9/10] END ...................n_neighbors=20;, score=0.845 total time=   5.2s\n",
      "[CV 10/10] END ..................n_neighbors=20;, score=0.853 total time=   5.0s\n",
      "Training time 230.9560155979998 seconds\n",
      "The best Accuracy of all model parameters' combination on model: 0.8426\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061884</td>\n",
       "      <td>5.622347</td>\n",
       "      <td>{'n_neighbors': 3}</td>\n",
       "      <td>0.767553</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.064161</td>\n",
       "      <td>5.191109</td>\n",
       "      <td>{'n_neighbors': 5}</td>\n",
       "      <td>0.789436</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.067887</td>\n",
       "      <td>6.020517</td>\n",
       "      <td>{'n_neighbors': 10}</td>\n",
       "      <td>0.825741</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.068337</td>\n",
       "      <td>5.987792</td>\n",
       "      <td>{'n_neighbors': 20}</td>\n",
       "      <td>0.842582</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time               params  mean_test_score  \\\n",
       "0       0.061884         5.622347   {'n_neighbors': 3}         0.767553   \n",
       "1       0.064161         5.191109   {'n_neighbors': 5}         0.789436   \n",
       "2       0.067887         6.020517  {'n_neighbors': 10}         0.825741   \n",
       "3       0.068337         5.987792  {'n_neighbors': 20}         0.842582   \n",
       "\n",
       "   rank_test_score  \n",
       "0                4  \n",
       "1                3  \n",
       "2                2  \n",
       "3                1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 0.0462652709975373 seconds\n",
      "Performance of optimized model on training dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[16356  3402]\n",
      " [ 1904 18003]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.83      0.86     19758\n",
      "    positive       0.84      0.90      0.87     19907\n",
      "\n",
      "    accuracy                           0.87     39665\n",
      "   macro avg       0.87      0.87      0.87     39665\n",
      "weighted avg       0.87      0.87      0.87     39665\n",
      "\n",
      "\n",
      "accuracy: 0.866\n",
      "f1 score: 0.866\n",
      "the accuracy of the optimized model on testing dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[3927 1013]\n",
      " [ 543 4434]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.88      0.79      0.83      4940\n",
      "    positive       0.81      0.89      0.85      4977\n",
      "\n",
      "    accuracy                           0.84      9917\n",
      "   macro avg       0.85      0.84      0.84      9917\n",
      "weighted avg       0.85      0.84      0.84      9917\n",
      "\n",
      "\n",
      "accuracy: 0.843\n",
      "f1 score: 0.843\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "params_log = {\"n_neighbors\": [5, 50, 100, 200]}\n",
    "\n",
    "grid_log = GridSearchCV(model, param_grid=params_log, scoring='accuracy', n_jobs=core_workers-1, cv=5, verbose=4)\n",
    "\n",
    "# Train model\n",
    "start = timeit.default_timer()\n",
    "grid_log.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time {} seconds'.format(str(stop - start)))\n",
    "print(\"The best Accuracy of all model parameters' combination on model: {:.4f}\".format(grid_log.best_score_))\n",
    "\n",
    "display(pd.DataFrame(grid_log.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_train_score', 'mean_test_score', 'rank_test_score']])\n",
    "\n",
    "# Retrain on best model\n",
    "best_model = grid_log.best_estimator_\n",
    "start = timeit.default_timer()\n",
    "best_model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time {} seconds'.format(str(stop - start)))\n",
    "\n",
    "# Score on train dataset.\n",
    "print('Performance of optimized model on training dataset...')\n",
    "_, _ = evaluate_model(best_model, train_X, train_y, label_names=None)\n",
    "\n",
    "# Score on test dataset.\n",
    "print(\"the accuracy of the optimized model on testing dataset...\")\n",
    "_, _ = evaluate_model(best_model, test_X, test_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-Search for Best Hyper-Parameters on Weighted K-NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV 1/5] END .....................n_neighbors=5;, score=0.781 total time=   9.6s\n",
      "[CV 2/5] END .....................n_neighbors=5;, score=0.794 total time=   9.9s\n",
      "[CV 3/5] END .....................n_neighbors=5;, score=0.784 total time=  10.8s\n",
      "[CV 4/5] END .....................n_neighbors=5;, score=0.782 total time=  10.7s\n",
      "[CV 5/5] END .....................n_neighbors=5;, score=0.791 total time=  10.8s\n",
      "[CV 1/5] END ....................n_neighbors=25;, score=0.825 total time=  10.1s\n",
      "[CV 2/5] END ....................n_neighbors=25;, score=0.842 total time=  10.5s\n",
      "[CV 3/5] END ....................n_neighbors=25;, score=0.838 total time=  12.1s\n",
      "[CV 4/5] END ....................n_neighbors=25;, score=0.841 total time=  11.4s\n",
      "[CV 5/5] END ....................n_neighbors=25;, score=0.844 total time=  11.7s\n",
      "[CV 1/5] END ....................n_neighbors=75;, score=0.848 total time=  11.9s\n",
      "[CV 2/5] END ....................n_neighbors=75;, score=0.858 total time=  11.1s\n",
      "[CV 3/5] END ....................n_neighbors=75;, score=0.853 total time=  11.6s\n",
      "[CV 4/5] END ....................n_neighbors=75;, score=0.856 total time=   8.7s\n",
      "[CV 5/5] END ....................n_neighbors=75;, score=0.856 total time=   8.1s\n",
      "[CV 1/5] END ...................n_neighbors=100;, score=0.849 total time=   8.0s\n",
      "[CV 2/5] END ...................n_neighbors=100;, score=0.857 total time=   7.9s\n",
      "[CV 3/5] END ...................n_neighbors=100;, score=0.852 total time=   8.2s\n",
      "[CV 4/5] END ...................n_neighbors=100;, score=0.858 total time=   8.3s\n",
      "[CV 5/5] END ...................n_neighbors=100;, score=0.858 total time=   8.8s\n",
      "Training time 200.39445093399263 seconds\n",
      "The best Accuracy of all model parameters' combination on model: 0.8550\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.056721</td>\n",
       "      <td>8.169462</td>\n",
       "      <td>{'n_neighbors': 100}</td>\n",
       "      <td>0.855036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.056425</td>\n",
       "      <td>10.220739</td>\n",
       "      <td>{'n_neighbors': 75}</td>\n",
       "      <td>0.854027</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.056118</td>\n",
       "      <td>11.121811</td>\n",
       "      <td>{'n_neighbors': 25}</td>\n",
       "      <td>0.838170</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.056029</td>\n",
       "      <td>10.321029</td>\n",
       "      <td>{'n_neighbors': 5}</td>\n",
       "      <td>0.786411</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time                params  mean_test_score  \\\n",
       "3       0.056721         8.169462  {'n_neighbors': 100}         0.855036   \n",
       "2       0.056425        10.220739   {'n_neighbors': 75}         0.854027   \n",
       "1       0.056118        11.121811   {'n_neighbors': 25}         0.838170   \n",
       "0       0.056029        10.321029    {'n_neighbors': 5}         0.786411   \n",
       "\n",
       "   rank_test_score  \n",
       "3                1  \n",
       "2                2  \n",
       "1                3  \n",
       "0                4  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 0.04411939899728168 seconds\n",
      "Performance of optimized model on training dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[19758     0]\n",
      " [    0 19907]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       1.00      1.00      1.00     19758\n",
      "    positive       1.00      1.00      1.00     19907\n",
      "\n",
      "    accuracy                           1.00     39665\n",
      "   macro avg       1.00      1.00      1.00     39665\n",
      "weighted avg       1.00      1.00      1.00     39665\n",
      "\n",
      "\n",
      "accuracy: 1.000\n",
      "f1 score: 1.000\n",
      "the accuracy of the optimized model on testing dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4008  932]\n",
      " [ 517 4460]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.81      0.85      4940\n",
      "    positive       0.83      0.90      0.86      4977\n",
      "\n",
      "    accuracy                           0.85      9917\n",
      "   macro avg       0.86      0.85      0.85      9917\n",
      "weighted avg       0.86      0.85      0.85      9917\n",
      "\n",
      "\n",
      "accuracy: 0.854\n",
      "f1 score: 0.854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(weights='distance')\n",
    "params_log = {\"n_neighbors\": [5, 50, 100, 200]}\n",
    "\n",
    "grid_log = GridSearchCV(model, param_grid=params_log, scoring='accuracy', n_jobs=core_workers-1, cv=5, verbose=4)\n",
    "\n",
    "# Train model\n",
    "start = timeit.default_timer()\n",
    "grid_log.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time {} seconds'.format(str(stop - start)))\n",
    "print(\"The best Accuracy of all model parameters' combination on model: {:.4f}\".format(grid_log.best_score_))\n",
    "\n",
    "display(pd.DataFrame(grid_log.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_train_score', 'mean_test_score', 'rank_test_score']])\n",
    "\n",
    "# Retrain on best model\n",
    "best_model = grid_log.best_estimator_\n",
    "start = timeit.default_timer()\n",
    "best_model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time {} seconds'.format(str(stop - start)))\n",
    "\n",
    "# Score on train dataset.\n",
    "print('Performance of optimized model on training dataset...')\n",
    "_, _ = evaluate_model(best_model, train_X, train_y, label_names=None)\n",
    "\n",
    "# Score on test dataset.\n",
    "print(\"the accuracy of the optimized model on testing dataset...\")\n",
    "_, _ = evaluate_model(best_model, test_X, test_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-Search for Best Hyper-Parameters on Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.335814</td>\n",
       "      <td>0.120053</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 100}</td>\n",
       "      <td>0.995115</td>\n",
       "      <td>0.842456</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.472580</td>\n",
       "      <td>0.132345</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>0.939493</td>\n",
       "      <td>0.836027</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.456581</td>\n",
       "      <td>0.128486</td>\n",
       "      <td>{'max_depth': 8, 'n_estimators': 100}</td>\n",
       "      <td>0.880865</td>\n",
       "      <td>0.830077</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  mean_score_time                                  params  \\\n",
       "2      16.335814         0.120053  {'max_depth': 15, 'n_estimators': 100}   \n",
       "1      12.472580         0.132345  {'max_depth': 10, 'n_estimators': 100}   \n",
       "0      10.456581         0.128486   {'max_depth': 8, 'n_estimators': 100}   \n",
       "\n",
       "   mean_train_score  mean_test_score  rank_test_score  \n",
       "2          0.995115         0.842456                1  \n",
       "1          0.939493         0.836027                2  \n",
       "0          0.880865         0.830077                3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time 16.123889790003886 seconds\n",
      "Performance of optimized model on training dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[19714    44]\n",
      " [  212 19695]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.99      1.00      0.99     19758\n",
      "    positive       1.00      0.99      0.99     19907\n",
      "\n",
      "    accuracy                           0.99     39665\n",
      "   macro avg       0.99      0.99      0.99     39665\n",
      "weighted avg       0.99      0.99      0.99     39665\n",
      "\n",
      "\n",
      "accuracy: 0.994\n",
      "f1 score: 0.994\n",
      "the accuracy of the optimized model on testing dataset...\n",
      "Score on dataset...\n",
      "\n",
      "Confusion Matrix:\n",
      " [[4066  874]\n",
      " [ 643 4334]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.86      0.82      0.84      4940\n",
      "    positive       0.83      0.87      0.85      4977\n",
      "\n",
      "    accuracy                           0.85      9917\n",
      "   macro avg       0.85      0.85      0.85      9917\n",
      "weighted avg       0.85      0.85      0.85      9917\n",
      "\n",
      "\n",
      "accuracy: 0.847\n",
      "f1 score: 0.847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_jobs=core_workers, random_state=random_state)\n",
    "params_log = {\"n_estimators\": [100], 'max_depth': [8, 10, 15]}\n",
    "\n",
    "grid_log = GridSearchCV(model, param_grid=params_log, scoring='accuracy', n_jobs=None, cv=5, verbose=4, return_train_score=True)\n",
    "\n",
    "# Train model\n",
    "start = timeit.default_timer()\n",
    "grid_log.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time {} seconds'.format(str(stop - start)))\n",
    "print(\"The best Accuracy of all model parameters' combination on model: {:.4f}\".format(grid_log.best_score_))\n",
    "\n",
    "display(pd.DataFrame(grid_log.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_train_score', 'mean_test_score', 'rank_test_score']])\n",
    "\n",
    "# Retrain on best model\n",
    "best_model = grid_log.best_estimator_\n",
    "start = timeit.default_timer()\n",
    "best_model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time {} seconds'.format(str(stop - start)))\n",
    "\n",
    "# Score on train dataset.\n",
    "print('Performance of optimized model on training dataset...')\n",
    "_, _ = evaluate_model(best_model, train_X, train_y, label_names=None)\n",
    "\n",
    "# Score on test dataset.\n",
    "print(\"the accuracy of the optimized model on testing dataset...\")\n",
    "_, _ = evaluate_model(best_model, test_X, test_y, label_names=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid-Search for Best Hyper-Parameters on SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV 1/5] END .........C=0.5;, score=(train=0.871, test=0.863) total time=  31.6s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_106822/4198069269.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mgrid_log\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimeit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_timer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training time {} seconds'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstop\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    836\u001b[0m                     )\n\u001b[1;32m    837\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 838\u001b[0;31m                 out = parallel(\n\u001b[0m\u001b[1;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[1;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1044\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1045\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1046\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    857\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    775\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 777\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    778\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/utils/fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[1;32m    679\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/svm/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    255\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/pythonProject/venv/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0msolver_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_liblinear_solver_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpenalty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1179\u001b[0;31m     raw_coef_, n_iter_ = liblinear.train_wrap(\n\u001b[0m\u001b[1;32m   1180\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1181\u001b[0m         \u001b[0my_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "model = LinearSVC(random_state=random_state)\n",
    "params_log = {'C': [0.5, 1, 5,10]}\n",
    "\n",
    "grid_log = GridSearchCV(model, param_grid=params_log, scoring='accuracy', n_jobs=None, cv=5, verbose=4, return_train_score=True)\n",
    "\n",
    "# Train model\n",
    "start = timeit.default_timer()\n",
    "grid_log.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time {} seconds'.format(str(stop - start)))\n",
    "print(\"The best Accuracy of all model parameters' combination on model: {:.4f}\".format(grid_log.best_score_))\n",
    "\n",
    "display(pd.DataFrame(grid_log.cv_results_).sort_values(by=['rank_test_score'])[['mean_fit_time', 'mean_score_time', \n",
    "                                           'params', 'mean_train_score', 'mean_test_score', 'rank_test_score']])\n",
    "\n",
    "# Retrain on best model\n",
    "best_model = grid_log.best_estimator_\n",
    "start = timeit.default_timer()\n",
    "best_model.fit(train_X, train_y)\n",
    "stop = timeit.default_timer()\n",
    "print('Training time {} seconds'.format(str(stop - start)))\n",
    "\n",
    "# Score on train dataset.\n",
    "print('Performance of optimized model on training dataset...')\n",
    "_, _ = evaluate_model(best_model, train_X, train_y, label_names=None)\n",
    "\n",
    "# Score on test dataset.\n",
    "print(\"the accuracy of the optimized model on testing dataset...\")\n",
    "_, _ = evaluate_model(best_model, test_X, test_y, label_names=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
